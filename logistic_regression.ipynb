{
 "cells": [
  {
   "cell_type": "code",
   "id": "0f880a9c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T15:23:34.659496Z",
     "start_time": "2025-11-28T15:23:34.511845Z"
    }
   },
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import polars as pl"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "ac77e339a8b46e1e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T15:23:34.675297Z",
     "start_time": "2025-11-28T15:23:34.664191Z"
    }
   },
   "source": [
    "SEED = 462\n",
    "np.random.seed(SEED)"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "5240eb93449fa9bc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T15:23:34.712596Z",
     "start_time": "2025-11-28T15:23:34.709863Z"
    }
   },
   "source": [
    "data_path = os.path.join(\"data\", \"tabular\")"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "9a21560b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T15:23:34.760942Z",
     "start_time": "2025-11-28T15:23:34.757013Z"
    }
   },
   "source": [
    "class Dataset:\n",
    "    def __init__(self, train_path, val_path, test_path):\n",
    "        self.train_path = train_path\n",
    "        self.val_path = val_path\n",
    "        self.test_path = test_path\n",
    "        self.mean = None\n",
    "        self.std = None\n",
    "        self.label_map = None\n",
    "\n",
    "    def load_csv(self, path):\n",
    "        df = pl.read_csv(path)\n",
    "        data = df.to_numpy()\n",
    "        X = data[:, :-1].astype(float)\n",
    "        Y_str = data[:, -1]\n",
    "        return X, Y_str\n",
    "\n",
    "    def encode_labels(self, Y_str, fit=False):\n",
    "        if fit:\n",
    "            unique_labels = np.unique(Y_str)\n",
    "            self.label_map = {label: i for i, label in enumerate(unique_labels)}\n",
    "\n",
    "        Y = np.array([self.label_map[label] for label in Y_str])\n",
    "        return Y\n",
    "\n",
    "    def normalize(self, X, fit=False):\n",
    "        if fit:\n",
    "            self.mean = np.mean(X, axis=0)\n",
    "            self.std = np.std(X, axis=0)\n",
    "            self.std[self.std == 0] = 1.0\n",
    "\n",
    "        return (X - self.mean) / self.std\n",
    "\n",
    "    def get_data(self):\n",
    "        X_train, Y_train_str = self.load_csv(self.train_path)\n",
    "        X_val, Y_val_str = self.load_csv(self.val_path)\n",
    "        X_test, Y_test_str = self.load_csv(self.test_path)\n",
    "\n",
    "        Y_train = self.encode_labels(Y_train_str, fit=True)\n",
    "        Y_val = self.encode_labels(Y_val_str, fit=False)\n",
    "        Y_test = self.encode_labels(Y_test_str, fit=False)\n",
    "\n",
    "        X_train = self.normalize(X_train, fit=True)\n",
    "        X_val = self.normalize(X_val, fit=False)\n",
    "        X_test = self.normalize(X_test, fit=False)\n",
    "\n",
    "        return (X_train, Y_train), (X_val, Y_val), (X_test, Y_test)"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "9d437f66",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T15:23:34.809642Z",
     "start_time": "2025-11-28T15:23:34.806802Z"
    }
   },
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self, learning_rate, num_iters):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_iters = num_iters\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        z = np.clip(z, -500, 500)\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        return self.sigmoid(np.dot(X, self.weights) + self.bias)\n",
    "\n",
    "    def logistic_loss(self, y_true, y_pred):\n",
    "        eps = 1e-15\n",
    "        y_pred = np.clip(y_pred, eps, 1 - eps)\n",
    "        return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "\n",
    "    def train(self, X, Y):\n",
    "        n_examples, n_features = X.shape\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "\n",
    "        for i in range(self.num_iters):\n",
    "            y_pred = self.predict_proba(X)\n",
    "            y_diff = y_pred - Y\n",
    "\n",
    "            self.weights -= self.learning_rate * np.dot(X.T, y_diff) / n_examples\n",
    "            self.bias -= self.learning_rate * np.mean(y_diff)"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "5ad20652",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T15:23:34.856842Z",
     "start_time": "2025-11-28T15:23:34.852941Z"
    }
   },
   "source": [
    "class LogisticRegressionOVA:\n",
    "    def __init__(self, learning_rate=0.01, num_iters=1000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_iters = num_iters\n",
    "        self.models = []\n",
    "        self.classes = None\n",
    "\n",
    "    def train(self, X_train, Y_train, X_val, Y_val):\n",
    "        self.classes = np.unique(Y_train)\n",
    "        self.models = []\n",
    "\n",
    "        for cls in self.classes:\n",
    "            print(f\"Training for class {cls}...\")\n",
    "            Y_train_bin = (Y_train == cls).astype(float)\n",
    "            Y_val_bin = (Y_val == cls).astype(float)\n",
    "\n",
    "            model = LogisticRegression(self.learning_rate, self.num_iters)\n",
    "\n",
    "            # Custom training loop to print loss\n",
    "            n_examples, n_features = X_train.shape\n",
    "            model.weights = np.zeros(n_features)\n",
    "            model.bias = 0\n",
    "\n",
    "            for i in range(model.num_iters):\n",
    "                y_pred = model.predict_proba(X_train)\n",
    "                y_diff = y_pred - Y_train_bin\n",
    "\n",
    "                model.weights -= model.learning_rate * np.dot(X_train.T, y_diff) / n_examples\n",
    "                model.bias -= model.learning_rate * np.mean(y_diff)\n",
    "\n",
    "                if i % 1000 == 0:\n",
    "                    train_loss = model.logistic_loss(Y_train_bin, y_pred)\n",
    "                    val_pred = model.predict_proba(X_val)\n",
    "                    val_loss = model.logistic_loss(Y_val_bin, val_pred)\n",
    "                    print(f\"Iter {i}: Train Loss {train_loss:.4f}, Val Loss {val_loss:.4f}\")\n",
    "\n",
    "            self.models.append(model)\n",
    "\n",
    "    def predict(self, X):\n",
    "        probs = np.column_stack([model.predict_proba(X) for model in self.models])\n",
    "        return np.argmax(probs, axis=1)\n",
    "\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    return np.mean(y_true == y_pred) * 100"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "76d20260",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T15:23:34.902527Z",
     "start_time": "2025-11-28T15:23:34.900557Z"
    }
   },
   "source": [
    "def accuracy(y_true, y_pred):\n",
    "    return np.mean(y_true == y_pred) * 100"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "f4383e78",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T15:23:47.695593Z",
     "start_time": "2025-11-28T15:23:34.946022Z"
    }
   },
   "source": [
    "if __name__ == \"__main__\":\n",
    "    dataset = Dataset(\n",
    "        train_path=os.path.join(data_path, \"train_processed.csv\"),\n",
    "        val_path=os.path.join(data_path, \"validation_processed.csv\"),\n",
    "        test_path=os.path.join(data_path, \"test_processed.csv\"),\n",
    "    )\n",
    "\n",
    "    (X_train, Y_train), (X_val, Y_val), (X_test, Y_test) = dataset.get_data()\n",
    "\n",
    "    model = LogisticRegressionOVA(learning_rate=0.001, num_iters=50000)\n",
    "    model.train(X_train, Y_train, X_val, Y_val)\n",
    "\n",
    "    test_pred = model.predict(X_test)\n",
    "    print(f\"Test Accuracy: {accuracy(Y_test, test_pred):.2f}%\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for class 0...\n",
      "Iter 0: Train Loss 0.6931, Val Loss 0.6929\n",
      "Iter 1000: Train Loss 0.5217, Val Loss 0.5201\n",
      "Iter 2000: Train Loss 0.4256, Val Loss 0.4233\n",
      "Iter 3000: Train Loss 0.3664, Val Loss 0.3636\n",
      "Iter 4000: Train Loss 0.3268, Val Loss 0.3237\n",
      "Iter 5000: Train Loss 0.2987, Val Loss 0.2953\n",
      "Iter 6000: Train Loss 0.2777, Val Loss 0.2740\n",
      "Iter 7000: Train Loss 0.2615, Val Loss 0.2576\n",
      "Iter 8000: Train Loss 0.2485, Val Loss 0.2445\n",
      "Iter 9000: Train Loss 0.2380, Val Loss 0.2339\n",
      "Iter 10000: Train Loss 0.2292, Val Loss 0.2250\n",
      "Iter 11000: Train Loss 0.2218, Val Loss 0.2175\n",
      "Iter 12000: Train Loss 0.2154, Val Loss 0.2111\n",
      "Iter 13000: Train Loss 0.2099, Val Loss 0.2055\n",
      "Iter 14000: Train Loss 0.2050, Val Loss 0.2006\n",
      "Iter 15000: Train Loss 0.2007, Val Loss 0.1963\n",
      "Iter 16000: Train Loss 0.1969, Val Loss 0.1924\n",
      "Iter 17000: Train Loss 0.1934, Val Loss 0.1889\n",
      "Iter 18000: Train Loss 0.1903, Val Loss 0.1858\n",
      "Iter 19000: Train Loss 0.1875, Val Loss 0.1830\n",
      "Iter 20000: Train Loss 0.1848, Val Loss 0.1804\n",
      "Iter 21000: Train Loss 0.1824, Val Loss 0.1780\n",
      "Iter 22000: Train Loss 0.1802, Val Loss 0.1758\n",
      "Iter 23000: Train Loss 0.1782, Val Loss 0.1738\n",
      "Iter 24000: Train Loss 0.1763, Val Loss 0.1719\n",
      "Iter 25000: Train Loss 0.1745, Val Loss 0.1701\n",
      "Iter 26000: Train Loss 0.1728, Val Loss 0.1685\n",
      "Iter 27000: Train Loss 0.1713, Val Loss 0.1670\n",
      "Iter 28000: Train Loss 0.1698, Val Loss 0.1655\n",
      "Iter 29000: Train Loss 0.1685, Val Loss 0.1642\n",
      "Iter 30000: Train Loss 0.1672, Val Loss 0.1629\n",
      "Iter 31000: Train Loss 0.1659, Val Loss 0.1617\n",
      "Iter 32000: Train Loss 0.1648, Val Loss 0.1606\n",
      "Iter 33000: Train Loss 0.1637, Val Loss 0.1595\n",
      "Iter 34000: Train Loss 0.1626, Val Loss 0.1585\n",
      "Iter 35000: Train Loss 0.1616, Val Loss 0.1575\n",
      "Iter 36000: Train Loss 0.1607, Val Loss 0.1566\n",
      "Iter 37000: Train Loss 0.1597, Val Loss 0.1557\n",
      "Iter 38000: Train Loss 0.1589, Val Loss 0.1548\n",
      "Iter 39000: Train Loss 0.1580, Val Loss 0.1540\n",
      "Iter 40000: Train Loss 0.1572, Val Loss 0.1533\n",
      "Iter 41000: Train Loss 0.1565, Val Loss 0.1525\n",
      "Iter 42000: Train Loss 0.1558, Val Loss 0.1518\n",
      "Iter 43000: Train Loss 0.1550, Val Loss 0.1511\n",
      "Iter 44000: Train Loss 0.1544, Val Loss 0.1505\n",
      "Iter 45000: Train Loss 0.1537, Val Loss 0.1498\n",
      "Iter 46000: Train Loss 0.1531, Val Loss 0.1492\n",
      "Iter 47000: Train Loss 0.1525, Val Loss 0.1487\n",
      "Iter 48000: Train Loss 0.1519, Val Loss 0.1481\n",
      "Iter 49000: Train Loss 0.1513, Val Loss 0.1475\n",
      "Training for class 1...\n",
      "Iter 0: Train Loss 0.6931, Val Loss 0.6929\n",
      "Iter 1000: Train Loss 0.5092, Val Loss 0.5125\n",
      "Iter 2000: Train Loss 0.4108, Val Loss 0.4163\n",
      "Iter 3000: Train Loss 0.3517, Val Loss 0.3585\n",
      "Iter 4000: Train Loss 0.3126, Val Loss 0.3203\n",
      "Iter 5000: Train Loss 0.2849, Val Loss 0.2933\n",
      "Iter 6000: Train Loss 0.2643, Val Loss 0.2731\n",
      "Iter 7000: Train Loss 0.2483, Val Loss 0.2575\n",
      "Iter 8000: Train Loss 0.2356, Val Loss 0.2451\n",
      "Iter 9000: Train Loss 0.2252, Val Loss 0.2349\n",
      "Iter 10000: Train Loss 0.2165, Val Loss 0.2264\n",
      "Iter 11000: Train Loss 0.2092, Val Loss 0.2191\n",
      "Iter 12000: Train Loss 0.2029, Val Loss 0.2129\n",
      "Iter 13000: Train Loss 0.1975, Val Loss 0.2076\n",
      "Iter 14000: Train Loss 0.1927, Val Loss 0.2028\n",
      "Iter 15000: Train Loss 0.1885, Val Loss 0.1986\n",
      "Iter 16000: Train Loss 0.1847, Val Loss 0.1949\n",
      "Iter 17000: Train Loss 0.1814, Val Loss 0.1915\n",
      "Iter 18000: Train Loss 0.1783, Val Loss 0.1884\n",
      "Iter 19000: Train Loss 0.1755, Val Loss 0.1857\n",
      "Iter 20000: Train Loss 0.1730, Val Loss 0.1831\n",
      "Iter 21000: Train Loss 0.1707, Val Loss 0.1808\n",
      "Iter 22000: Train Loss 0.1686, Val Loss 0.1786\n",
      "Iter 23000: Train Loss 0.1666, Val Loss 0.1767\n",
      "Iter 24000: Train Loss 0.1648, Val Loss 0.1748\n",
      "Iter 25000: Train Loss 0.1631, Val Loss 0.1731\n",
      "Iter 26000: Train Loss 0.1615, Val Loss 0.1715\n",
      "Iter 27000: Train Loss 0.1600, Val Loss 0.1700\n",
      "Iter 28000: Train Loss 0.1587, Val Loss 0.1686\n",
      "Iter 29000: Train Loss 0.1574, Val Loss 0.1672\n",
      "Iter 30000: Train Loss 0.1561, Val Loss 0.1660\n",
      "Iter 31000: Train Loss 0.1550, Val Loss 0.1648\n",
      "Iter 32000: Train Loss 0.1539, Val Loss 0.1637\n",
      "Iter 33000: Train Loss 0.1529, Val Loss 0.1626\n",
      "Iter 34000: Train Loss 0.1519, Val Loss 0.1616\n",
      "Iter 35000: Train Loss 0.1510, Val Loss 0.1606\n",
      "Iter 36000: Train Loss 0.1501, Val Loss 0.1597\n",
      "Iter 37000: Train Loss 0.1492, Val Loss 0.1589\n",
      "Iter 38000: Train Loss 0.1484, Val Loss 0.1580\n",
      "Iter 39000: Train Loss 0.1477, Val Loss 0.1572\n",
      "Iter 40000: Train Loss 0.1469, Val Loss 0.1565\n",
      "Iter 41000: Train Loss 0.1462, Val Loss 0.1558\n",
      "Iter 42000: Train Loss 0.1456, Val Loss 0.1551\n",
      "Iter 43000: Train Loss 0.1449, Val Loss 0.1544\n",
      "Iter 44000: Train Loss 0.1443, Val Loss 0.1537\n",
      "Iter 45000: Train Loss 0.1437, Val Loss 0.1531\n",
      "Iter 46000: Train Loss 0.1432, Val Loss 0.1525\n",
      "Iter 47000: Train Loss 0.1426, Val Loss 0.1520\n",
      "Iter 48000: Train Loss 0.1421, Val Loss 0.1514\n",
      "Iter 49000: Train Loss 0.1416, Val Loss 0.1509\n",
      "Training for class 2...\n",
      "Iter 0: Train Loss 0.6931, Val Loss 0.6929\n",
      "Iter 1000: Train Loss 0.4940, Val Loss 0.4978\n",
      "Iter 2000: Train Loss 0.3870, Val Loss 0.3926\n",
      "Iter 3000: Train Loss 0.3224, Val Loss 0.3290\n",
      "Iter 4000: Train Loss 0.2798, Val Loss 0.2869\n",
      "Iter 5000: Train Loss 0.2496, Val Loss 0.2572\n",
      "Iter 6000: Train Loss 0.2273, Val Loss 0.2352\n",
      "Iter 7000: Train Loss 0.2101, Val Loss 0.2182\n",
      "Iter 8000: Train Loss 0.1965, Val Loss 0.2048\n",
      "Iter 9000: Train Loss 0.1855, Val Loss 0.1939\n",
      "Iter 10000: Train Loss 0.1764, Val Loss 0.1849\n",
      "Iter 11000: Train Loss 0.1687, Val Loss 0.1773\n",
      "Iter 12000: Train Loss 0.1622, Val Loss 0.1708\n",
      "Iter 13000: Train Loss 0.1565, Val Loss 0.1652\n",
      "Iter 14000: Train Loss 0.1516, Val Loss 0.1603\n",
      "Iter 15000: Train Loss 0.1472, Val Loss 0.1560\n",
      "Iter 16000: Train Loss 0.1433, Val Loss 0.1521\n",
      "Iter 17000: Train Loss 0.1399, Val Loss 0.1486\n",
      "Iter 18000: Train Loss 0.1367, Val Loss 0.1455\n",
      "Iter 19000: Train Loss 0.1339, Val Loss 0.1427\n",
      "Iter 20000: Train Loss 0.1313, Val Loss 0.1401\n",
      "Iter 21000: Train Loss 0.1289, Val Loss 0.1377\n",
      "Iter 22000: Train Loss 0.1267, Val Loss 0.1355\n",
      "Iter 23000: Train Loss 0.1247, Val Loss 0.1335\n",
      "Iter 24000: Train Loss 0.1229, Val Loss 0.1316\n",
      "Iter 25000: Train Loss 0.1211, Val Loss 0.1298\n",
      "Iter 26000: Train Loss 0.1195, Val Loss 0.1282\n",
      "Iter 27000: Train Loss 0.1180, Val Loss 0.1266\n",
      "Iter 28000: Train Loss 0.1165, Val Loss 0.1252\n",
      "Iter 29000: Train Loss 0.1152, Val Loss 0.1238\n",
      "Iter 30000: Train Loss 0.1139, Val Loss 0.1225\n",
      "Iter 31000: Train Loss 0.1127, Val Loss 0.1213\n",
      "Iter 32000: Train Loss 0.1116, Val Loss 0.1202\n",
      "Iter 33000: Train Loss 0.1105, Val Loss 0.1191\n",
      "Iter 34000: Train Loss 0.1095, Val Loss 0.1180\n",
      "Iter 35000: Train Loss 0.1086, Val Loss 0.1170\n",
      "Iter 36000: Train Loss 0.1076, Val Loss 0.1161\n",
      "Iter 37000: Train Loss 0.1067, Val Loss 0.1152\n",
      "Iter 38000: Train Loss 0.1059, Val Loss 0.1143\n",
      "Iter 39000: Train Loss 0.1051, Val Loss 0.1135\n",
      "Iter 40000: Train Loss 0.1043, Val Loss 0.1127\n",
      "Iter 41000: Train Loss 0.1036, Val Loss 0.1119\n",
      "Iter 42000: Train Loss 0.1029, Val Loss 0.1112\n",
      "Iter 43000: Train Loss 0.1022, Val Loss 0.1105\n",
      "Iter 44000: Train Loss 0.1015, Val Loss 0.1098\n",
      "Iter 45000: Train Loss 0.1009, Val Loss 0.1092\n",
      "Iter 46000: Train Loss 0.1003, Val Loss 0.1085\n",
      "Iter 47000: Train Loss 0.0997, Val Loss 0.1079\n",
      "Iter 48000: Train Loss 0.0991, Val Loss 0.1073\n",
      "Iter 49000: Train Loss 0.0986, Val Loss 0.1068\n",
      "Training for class 3...\n",
      "Iter 0: Train Loss 0.6931, Val Loss 0.6930\n",
      "Iter 1000: Train Loss 0.5526, Val Loss 0.5503\n",
      "Iter 2000: Train Loss 0.4710, Val Loss 0.4673\n",
      "Iter 3000: Train Loss 0.4192, Val Loss 0.4148\n",
      "Iter 4000: Train Loss 0.3840, Val Loss 0.3789\n",
      "Iter 5000: Train Loss 0.3586, Val Loss 0.3530\n",
      "Iter 6000: Train Loss 0.3394, Val Loss 0.3335\n",
      "Iter 7000: Train Loss 0.3244, Val Loss 0.3182\n",
      "Iter 8000: Train Loss 0.3124, Val Loss 0.3059\n",
      "Iter 9000: Train Loss 0.3025, Val Loss 0.2958\n",
      "Iter 10000: Train Loss 0.2943, Val Loss 0.2874\n",
      "Iter 11000: Train Loss 0.2873, Val Loss 0.2802\n",
      "Iter 12000: Train Loss 0.2813, Val Loss 0.2739\n",
      "Iter 13000: Train Loss 0.2760, Val Loss 0.2685\n",
      "Iter 14000: Train Loss 0.2714, Val Loss 0.2637\n",
      "Iter 15000: Train Loss 0.2673, Val Loss 0.2595\n",
      "Iter 16000: Train Loss 0.2636, Val Loss 0.2557\n",
      "Iter 17000: Train Loss 0.2603, Val Loss 0.2523\n",
      "Iter 18000: Train Loss 0.2574, Val Loss 0.2492\n",
      "Iter 19000: Train Loss 0.2546, Val Loss 0.2464\n",
      "Iter 20000: Train Loss 0.2522, Val Loss 0.2438\n",
      "Iter 21000: Train Loss 0.2499, Val Loss 0.2414\n",
      "Iter 22000: Train Loss 0.2478, Val Loss 0.2392\n",
      "Iter 23000: Train Loss 0.2458, Val Loss 0.2372\n",
      "Iter 24000: Train Loss 0.2440, Val Loss 0.2353\n",
      "Iter 25000: Train Loss 0.2424, Val Loss 0.2335\n",
      "Iter 26000: Train Loss 0.2408, Val Loss 0.2319\n",
      "Iter 27000: Train Loss 0.2393, Val Loss 0.2304\n",
      "Iter 28000: Train Loss 0.2380, Val Loss 0.2289\n",
      "Iter 29000: Train Loss 0.2367, Val Loss 0.2276\n",
      "Iter 30000: Train Loss 0.2355, Val Loss 0.2263\n",
      "Iter 31000: Train Loss 0.2343, Val Loss 0.2251\n",
      "Iter 32000: Train Loss 0.2332, Val Loss 0.2240\n",
      "Iter 33000: Train Loss 0.2322, Val Loss 0.2229\n",
      "Iter 34000: Train Loss 0.2312, Val Loss 0.2219\n",
      "Iter 35000: Train Loss 0.2303, Val Loss 0.2209\n",
      "Iter 36000: Train Loss 0.2294, Val Loss 0.2200\n",
      "Iter 37000: Train Loss 0.2286, Val Loss 0.2191\n",
      "Iter 38000: Train Loss 0.2278, Val Loss 0.2183\n",
      "Iter 39000: Train Loss 0.2270, Val Loss 0.2175\n",
      "Iter 40000: Train Loss 0.2263, Val Loss 0.2167\n",
      "Iter 41000: Train Loss 0.2256, Val Loss 0.2160\n",
      "Iter 42000: Train Loss 0.2250, Val Loss 0.2153\n",
      "Iter 43000: Train Loss 0.2243, Val Loss 0.2146\n",
      "Iter 44000: Train Loss 0.2237, Val Loss 0.2140\n",
      "Iter 45000: Train Loss 0.2231, Val Loss 0.2134\n",
      "Iter 46000: Train Loss 0.2225, Val Loss 0.2128\n",
      "Iter 47000: Train Loss 0.2220, Val Loss 0.2122\n",
      "Iter 48000: Train Loss 0.2215, Val Loss 0.2116\n",
      "Iter 49000: Train Loss 0.2210, Val Loss 0.2111\n",
      "Training for class 4...\n",
      "Iter 0: Train Loss 0.6931, Val Loss 0.6928\n",
      "Iter 1000: Train Loss 0.4660, Val Loss 0.4632\n",
      "Iter 2000: Train Loss 0.3547, Val Loss 0.3516\n",
      "Iter 3000: Train Loss 0.2913, Val Loss 0.2884\n",
      "Iter 4000: Train Loss 0.2506, Val Loss 0.2482\n",
      "Iter 5000: Train Loss 0.2224, Val Loss 0.2203\n",
      "Iter 6000: Train Loss 0.2016, Val Loss 0.1998\n",
      "Iter 7000: Train Loss 0.1856, Val Loss 0.1841\n",
      "Iter 8000: Train Loss 0.1730, Val Loss 0.1716\n",
      "Iter 9000: Train Loss 0.1627, Val Loss 0.1615\n",
      "Iter 10000: Train Loss 0.1541, Val Loss 0.1531\n",
      "Iter 11000: Train Loss 0.1469, Val Loss 0.1460\n",
      "Iter 12000: Train Loss 0.1407, Val Loss 0.1399\n",
      "Iter 13000: Train Loss 0.1354, Val Loss 0.1347\n",
      "Iter 14000: Train Loss 0.1307, Val Loss 0.1300\n",
      "Iter 15000: Train Loss 0.1265, Val Loss 0.1259\n",
      "Iter 16000: Train Loss 0.1228, Val Loss 0.1223\n",
      "Iter 17000: Train Loss 0.1195, Val Loss 0.1190\n",
      "Iter 18000: Train Loss 0.1165, Val Loss 0.1160\n",
      "Iter 19000: Train Loss 0.1138, Val Loss 0.1133\n",
      "Iter 20000: Train Loss 0.1113, Val Loss 0.1109\n",
      "Iter 21000: Train Loss 0.1090, Val Loss 0.1086\n",
      "Iter 22000: Train Loss 0.1069, Val Loss 0.1065\n",
      "Iter 23000: Train Loss 0.1050, Val Loss 0.1046\n",
      "Iter 24000: Train Loss 0.1032, Val Loss 0.1028\n",
      "Iter 25000: Train Loss 0.1015, Val Loss 0.1011\n",
      "Iter 26000: Train Loss 0.1000, Val Loss 0.0995\n",
      "Iter 27000: Train Loss 0.0985, Val Loss 0.0981\n",
      "Iter 28000: Train Loss 0.0971, Val Loss 0.0967\n",
      "Iter 29000: Train Loss 0.0958, Val Loss 0.0954\n",
      "Iter 30000: Train Loss 0.0946, Val Loss 0.0942\n",
      "Iter 31000: Train Loss 0.0934, Val Loss 0.0930\n",
      "Iter 32000: Train Loss 0.0924, Val Loss 0.0919\n",
      "Iter 33000: Train Loss 0.0913, Val Loss 0.0909\n",
      "Iter 34000: Train Loss 0.0903, Val Loss 0.0899\n",
      "Iter 35000: Train Loss 0.0894, Val Loss 0.0889\n",
      "Iter 36000: Train Loss 0.0885, Val Loss 0.0880\n",
      "Iter 37000: Train Loss 0.0877, Val Loss 0.0872\n",
      "Iter 38000: Train Loss 0.0868, Val Loss 0.0863\n",
      "Iter 39000: Train Loss 0.0861, Val Loss 0.0855\n",
      "Iter 40000: Train Loss 0.0853, Val Loss 0.0848\n",
      "Iter 41000: Train Loss 0.0846, Val Loss 0.0841\n",
      "Iter 42000: Train Loss 0.0839, Val Loss 0.0834\n",
      "Iter 43000: Train Loss 0.0832, Val Loss 0.0827\n",
      "Iter 44000: Train Loss 0.0826, Val Loss 0.0820\n",
      "Iter 45000: Train Loss 0.0820, Val Loss 0.0814\n",
      "Iter 46000: Train Loss 0.0814, Val Loss 0.0808\n",
      "Iter 47000: Train Loss 0.0808, Val Loss 0.0802\n",
      "Iter 48000: Train Loss 0.0803, Val Loss 0.0797\n",
      "Iter 49000: Train Loss 0.0798, Val Loss 0.0791\n",
      "Test Accuracy: 85.05%\n"
     ]
    }
   ],
   "execution_count": 8
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".mlvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
